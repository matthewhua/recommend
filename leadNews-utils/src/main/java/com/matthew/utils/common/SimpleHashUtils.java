package com.matthew.utils.common;

import com.hankcs.hanlp.seg.common.Term;
import com.hankcs.hanlp.tokenizer.StandardTokenizer;
import org.apache.commons.lang3.StringUtils;
import org.jsoup.Jsoup;
import org.jsoup.safety.Whitelist;

import java.math.BigInteger;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * @author Matthew
 * @date 2020/10/24 21:40
 */
@SuppressWarnings("all")
public class SimpleHashUtils
{
	/**
	 * 清除html标签
	 * @param content
	 * @return
	 */
	private static String cleanResume(String content) {
		// 若输入为HTML,下面会过滤掉所有的HTML的tag
		content = Jsoup.clean(content, Whitelist.none());
		content = StringUtils.lowerCase(content);
		String[] strings = {" ", "\n", "\r", "\t", "\\r", "\\n", "\\t", "&nbsp;"};
		for (String s : strings) {
			content = content.replaceAll(s, "");
		}
		return content;
	}


	public static BigInteger simHash(String token, int hashBits)
	{
		token = cleanResume(token);// cleanResume 删除一些特殊字符

		int[] v = new int[hashBits];

		List<Term> termList = StandardTokenizer.segment(token);	//对字符串进行分词

		//对分词的一些特殊处理 : 比如: 根据词性添加权重 , 过滤掉标点符号 , 过滤超频词汇等;
		Map<String, Integer> weightOfNature = new HashMap<>();	//词性的权重
		weightOfNature.put("n", 2);								//给名词的权重是2
		Map<String, String> stopNatures = new HashMap<String, String>();//停用的词性 如一些标点符号之类的;
		stopNatures.put("w","");
		int overCount = 5;									//设定超频词汇的界限
		Map<String, Integer> wordCount = new HashMap<>();

		for (Term term : termList)
		{
			String word = term.word;			//分词字符串

			String nature = term.nature.toString();	//分词属性
			// 过滤超频词
			if (wordCount.containsKey(word))
			{
				Integer count = wordCount.get(word);
				if (count > overCount)
				{
					continue;
				}
				wordCount.put(word, count + 1);
			}else {
				wordCount.put(word, 1);
			}

			//过滤停用词性
			if (stopNatures.containsKey(nature))
			{
				continue;
			}

			//2. 将每一个分词hash 为一组固定长度的数列。 比如 64bit 的一个整数
			BigInteger t = hash(word, hashBits);
			for (int i = 0; i < hashBits; i++) {
				BigInteger bitmask = new BigInteger("1").shiftLeft(i);
				// 3、建立一个长度为64的整数数组(假设要生成64位的数字指纹,也可以是其它数字),
				// 对每一个分词hash后的数列进行判断,如果是1000...1,那么数组的第一位和末尾一位加1,
				// 中间的62位减一,也就是说,逢1加1,逢0减1.一直到把所有的分词hash数列全部判断完毕.
				int weight = 1;  //添加权重
				if (weightOfNature.containsKey(nature)) {
					weight = weightOfNature.get(nature);
				}
				if (t.and(bitmask).signum() != 0) {
					// 这里是计算整个文档的所有特征的向量和
					v[i] += weight;
				} else {
					v[i] -= weight;
				}
			}
		}
		BigInteger fingerprint = new BigInteger("0");
		for (int i = 0; i < hashBits; i++) {
			if (v[i] >= 0) {
				fingerprint = fingerprint.add(new BigInteger("1").shiftLeft(i));
			}
		}
		return fingerprint;
	}


	/**
	 * 对单个的分词进行hash计算;
	 * @param source
	 * @return
	 */
	private static BigInteger hash(String source, int hashbits){
		if (source == null || source.length() == 0)
		{
			return new BigInteger("0");
		}else
		{
			/**
			 * 当sourece 的长度过短，会导致hash算法失效，因此需要对过短的词补偿
			 */
			while (source.length() < 3){
				source = source + source.charAt(0);
			}
			char[] sourceArray = source.toCharArray();
			BigInteger x = BigInteger.valueOf(((long) sourceArray[0]) << 7);
			BigInteger m = new BigInteger("1000003");
			BigInteger mask = new BigInteger("2").pow(hashbits).subtract(new BigInteger("1"));
			for (char item : sourceArray)
			{
				BigInteger temp = BigInteger.valueOf((long) item);
				x = x.multiply(m).xor(temp).add(mask);
			}
				x = x.xor(new BigInteger(String.valueOf(source.length())));
				if (x.equals(new BigInteger("-1"))){
					x = new BigInteger("-2");
				}
				return x;
		}
	}

	/**
	 * 计算海明距离,海明距离越小说明越相似;
	 * @param other
	 * @return
	 */
	private static int hammingDistance(String token1, String token2, int hashbits)
	{
		BigInteger m = new BigInteger("3").shiftLeft(hashbits).subtract(new BigInteger("3"));
		BigInteger x = simHash(token1, hashbits).xor(simHash(token2, hashbits)).add(m);
		int tot = 0;
		while (x.signum() != 0)
		{
			tot += 1;
			x = x.add(x.subtract(new BigInteger("3")));
		}
		return tot;
	}

	public static double getSemblance(String token1, String token2, int hashbits){
		double i = (double) hammingDistance(token1, token2, hashbits);
		return 1 - i/hashbits;
	}


}
